# ============================================================================
# PR√ÅCTICA HDFS CON PYSPARK EN AWS EMR
# Exploraci√≥n Pr√°ctica de HDFS con PySpark en AWS
# ============================================================================
# Duraci√≥n: 2-3 horas
# Objetivo: Aprender a usar HDFS en Amazon EMR integr√°ndolo con PySpark
# ============================================================================

"""
REQUISITOS PREVIOS:
-------------------
1. Acceso a AWS Academy Learner Lab
2. Cl√∫ster EMR configurado con Hadoop y Spark
3. Conexi√≥n SSH al nodo maestro (o uso de CloudShell)

ESTRUCTURA DE LA PR√ÅCTICA:
---------------------------
PARTE 1: Comandos HDFS (Terminal SSH)
PARTE 2: Trabajo con PySpark
PARTE 3: Cierre y Discusi√≥n
"""

# ============================================================================
# PARTE 1: COMANDOS HDFS
# ============================================================================
# Ejecutar estos comandos en la terminal SSH del nodo maestro EMR

"""
# ----------------------------------------------------------------------------
# PASO 3.1: CREAR DIRECTORIO EN HDFS
# ----------------------------------------------------------------------------

# Crear directorio para almacenar datos
hdfs dfs -mkdir /user/data

# Verificar que se cre√≥ correctamente
hdfs dfs -ls /user/


# ----------------------------------------------------------------------------
# PASO 3.2: CREAR Y SUBIR ARCHIVO A HDFS
# ----------------------------------------------------------------------------

# Crear un archivo CSV de ejemplo en el sistema local
cat > local_file.csv << 'EOF'
id,nombre,edad,ciudad,salario
1,Juan,28,Madrid,45000
2,Mar√≠a,34,Barcelona,52000
3,Carlos,45,Valencia,48000
4,Ana,29,Sevilla,41000
5,Pedro,38,Bilbao,55000
6,Laura,31,M√°laga,43000
7,Miguel,42,Zaragoza,50000
8,Carmen,27,Murcia,39000
9,David,36,Palma,47000
10,Elena,33,Granada,44000
EOF

# Verificar que el archivo se cre√≥ localmente
ls -lh local_file.csv
cat local_file.csv


# ----------------------------------------------------------------------------
# PASO 3.3: SUBIR ARCHIVO A HDFS
# ----------------------------------------------------------------------------

# Subir el archivo desde local a HDFS
hdfs dfs -put local_file.csv /user/data/

# Verificar que se subi√≥ correctamente
hdfs dfs -ls /user/data/

# Ver el contenido del archivo en HDFS
hdfs dfs -cat /user/data/local_file.csv


# ----------------------------------------------------------------------------
# COMANDOS √öTILES DE HDFS (Referencia r√°pida)
# ----------------------------------------------------------------------------

# Listar contenido de un directorio
hdfs dfs -ls /user/data/

# Ver las primeras l√≠neas de un archivo
hdfs dfs -head /user/data/local_file.csv

# Ver el contenido completo de un archivo
hdfs dfs -cat /user/data/local_file.csv

# Copiar archivo dentro de HDFS
hdfs dfs -cp /user/data/local_file.csv /user/data/backup.csv

# Mover/renombrar archivo en HDFS
hdfs dfs -mv /user/data/backup.csv /user/data/respaldo.csv

# Descargar archivo de HDFS a local
hdfs dfs -get /user/data/local_file.csv ./archivo_descargado.csv

# Eliminar archivo de HDFS
hdfs dfs -rm /user/data/respaldo.csv

# Crear directorio adicional
hdfs dfs -mkdir /user/data/temp

# Eliminar directorio vac√≠o
hdfs dfs -rmdir /user/data/temp

# Eliminar directorio con contenido (recursivo)
hdfs dfs -rm -r /user/data/temp/

# Ver espacio usado en HDFS
hdfs dfs -du -h /user/data/

# Ver informaci√≥n detallada de replicaci√≥n
hdfs fsck /user/data/local_file.csv -files -blocks -locations

# Ver estad√≠sticas del sistema HDFS
hdfs dfsadmin -report
"""

# ============================================================================
# PARTE 2: TRABAJO CON PYSPARK
# ============================================================================
# Ejecutar estos comandos despu√©s de iniciar PySpark con el comando: pyspark

print("="*80)
print("PARTE 2: TRABAJO CON PYSPARK")
print("="*80)

# ----------------------------------------------------------------------------
# PASO 4.1: INICIALIZACI√ìN Y CONFIGURACI√ìN
# ----------------------------------------------------------------------------

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg, count, when, sum as spark_sum, max as spark_max

# Crear sesi√≥n de Spark (si no est√° ya iniciada)
spark = SparkSession.builder \
    .appName("HDFS_PySpark_Practice") \
    .getOrCreate()

# Verificar la configuraci√≥n
print("\n" + "="*80)
print("CONFIGURACI√ìN DE SPARK")
print("="*80)
print(f"Versi√≥n de Spark: {spark.version}")
print(f"Master: {spark.sparkContext.master}")
print(f"App Name: {spark.sparkContext.appName}")

# ----------------------------------------------------------------------------
# PASO 4.2: LEER DATOS DESDE HDFS
# ----------------------------------------------------------------------------

print("\n" + "="*80)
print("PASO 4.2: LEER DATOS DESDE HDFS")
print("="*80)

# Leer el archivo CSV desde HDFS
df = spark.read.csv("hdfs:///user/data/local_file.csv", header=True, inferSchema=True)

# Mostrar los primeros registros
print("\nüìä DATOS ORIGINALES:")
df.show()

# Ver el esquema de datos
print("\nüìã ESQUEMA DE DATOS:")
df.printSchema()

# Informaci√≥n b√°sica del DataFrame
print("\nüìà INFORMACI√ìN B√ÅSICA:")
print(f"N√∫mero total de registros: {df.count()}")
print(f"N√∫mero de columnas: {len(df.columns)}")
print(f"Columnas: {df.columns}")

# Estad√≠sticas descriptivas
print("\nüìä ESTAD√çSTICAS DESCRIPTIVAS:")
df.describe().show()

# ----------------------------------------------------------------------------
# PASO 4.3: PROCESAR LOS DATOS (FILTROS Y TRANSFORMACIONES)
# ----------------------------------------------------------------------------

print("\n" + "="*80)
print("PASO 4.3: PROCESAR LOS DATOS")
print("="*80)

# Ejemplo de filtro: Empleados con salario mayor a 45000
print("\nüîç FILTRO: Empleados con salario > 45000")
filtered_df = df.filter(col("salario") > 45000)
filtered_df.show()

print(f"Total de empleados con salario > 45000: {filtered_df.count()}")

# Seleccionar columnas espec√≠ficas
print("\nüìå SELECCI√ìN DE COLUMNAS (nombre y salario):")
df.select("nombre", "salario").show()

# Ordenar por salario descendente
print("\nüìä EMPLEADOS ORDENADOS POR SALARIO (DESCENDENTE):")
df.orderBy(col("salario").desc()).show()

# Crear una nueva columna con categor√≠a de salario
print("\n‚ûï CREAR NUEVA COLUMNA: Categor√≠a de salario")
df_with_category = df.withColumn(
    "categoria_salario",
    when(col("salario") < 42000, "Bajo")
    .when((col("salario") >= 42000) & (col("salario") < 48000), "Medio")
    .otherwise("Alto")
)
df_with_category.show()

# Filtro m√∫ltiple: Empleados mayores de 30 a√±os con salario alto
print("\nüîç FILTRO M√öLTIPLE: Edad > 30 Y Salario > 47000")
df.filter((col("edad") > 30) & (col("salario") > 47000)).show()

# ----------------------------------------------------------------------------
# PASO 4.4: AN√ÅLISIS Y AGREGACIONES
# ----------------------------------------------------------------------------

print("\n" + "="*80)
print("AN√ÅLISIS Y AGREGACIONES")
print("="*80)

# Salario promedio por ciudad
print("\nüí∞ SALARIO PROMEDIO POR CIUDAD:")
df.groupBy("ciudad").avg("salario").show()

# Estad√≠sticas completas por ciudad
print("\nüìä ESTAD√çSTICAS COMPLETAS POR CIUDAD:")
city_stats = df.groupBy("ciudad").agg(
    count("*").alias("num_empleados"),
    avg("salario").alias("salario_promedio"),
    avg("edad").alias("edad_promedia"),
    spark_max("salario").alias("salario_maximo")
).orderBy(col("salario_promedio").desc())

city_stats.show()

# Contar empleados por categor√≠a de salario
print("\nüìà DISTRIBUCI√ìN POR CATEGOR√çA DE SALARIO:")
df_with_category.groupBy("categoria_salario").agg(
    count("*").alias("cantidad"),
    avg("salario").alias("salario_promedio")
).orderBy("categoria_salario").show()

# An√°lisis por rangos de edad
print("\nüë• AN√ÅLISIS POR RANGOS DE EDAD:")
df_age_ranges = df.withColumn(
    "rango_edad",
    when(col("edad") < 30, "20-29")
    .when((col("edad") >= 30) & (col("edad") < 40), "30-39")
    .otherwise("40+")
)

df_age_ranges.groupBy("rango_edad").agg(
    count("*").alias("cantidad"),
    avg("salario").alias("salario_promedio"),
    avg("edad").alias("edad_promedia")
).orderBy("rango_edad").show()

# Resumen general
print("\nüìä RESUMEN GENERAL:")
summary = df.agg(
    count("*").alias("total_empleados"),
    avg("edad").alias("edad_promedio"),
    avg("salario").alias("salario_promedio"),
    spark_max("salario").alias("salario_maximo"),
    spark_sum("salario").alias("masa_salarial_total")
)
summary.show()

# ----------------------------------------------------------------------------
# PASO 4.4: GUARDAR RESULTADOS EN HDFS
# ----------------------------------------------------------------------------

print("\n" + "="*80)
print("PASO 4.4: GUARDAR RESULTADOS EN HDFS")
print("="*80)

# Guardar datos filtrados en formato CSV
print("\nüíæ Guardando datos filtrados en CSV...")
filtered_df.write.mode("overwrite").option("header", "true").csv(
    "hdfs:///user/data/output_filtered"
)
print("‚úÖ Datos filtrados guardados en: hdfs:///user/data/output_filtered")

# Guardar datos con categor√≠as en formato Parquet
print("\nüíæ Guardando datos con categor√≠as en Parquet...")
df_with_category.write.mode("overwrite").parquet(
    "hdfs:///user/data/output_parquet"
)
print("‚úÖ Datos guardados en: hdfs:///user/data/output_parquet")

# Guardar estad√≠sticas por ciudad
print("\nüíæ Guardando estad√≠sticas por ciudad...")
city_stats.write.mode("overwrite").option("header", "true").csv(
    "hdfs:///user/data/city_statistics"
)
print("‚úÖ Estad√≠sticas guardadas en: hdfs:///user/data/city_statistics")

print("\nüéâ ¬°Todos los datos guardados exitosamente en HDFS!")

# ----------------------------------------------------------------------------
# VERIFICAR ARCHIVOS GUARDADOS
# ----------------------------------------------------------------------------

print("\n" + "="*80)
print("VERIFICAR ARCHIVOS GUARDADOS EN HDFS")
print("="*80)
print("\nEjecutar en terminal SSH:")
print("hdfs dfs -ls /user/data/")
print("hdfs dfs -ls /user/data/output_filtered/")
print("hdfs dfs -ls /user/data/output_parquet/")
print("hdfs dfs -cat /user/data/output_filtered/part-00000* | head -20")

# ----------------------------------------------------------------------------
# LEER DATOS GUARDADOS
# ----------------------------------------------------------------------------

print("\n" + "="*80)
print("LEER DATOS GUARDADOS DESDE HDFS")
print("="*80)

# Leer datos filtrados en CSV
print("\nüìÇ Leyendo datos filtrados (CSV):")
df_loaded_csv = spark.read.csv(
    "hdfs:///user/data/output_filtered",
    header=True,
    inferSchema=True
)
df_loaded_csv.show(5)

# Leer datos en formato Parquet
print("\nüìÇ Leyendo datos desde Parquet:")
df_loaded_parquet = spark.read.parquet("hdfs:///user/data/output_parquet")
df_loaded_parquet.show(5)

# Leer estad√≠sticas
print("\nüìÇ Leyendo estad√≠sticas por ciudad:")
df_stats = spark.read.csv(
    "hdfs:///user/data/city_statistics",
    header=True,
    inferSchema=True
)
df_stats.show()

print("\n‚úÖ ¬°Lectura de datos verificada correctamente!")

# ============================================================================
# PARTE 3: CIERRE Y DISCUSI√ìN
# ============================================================================

print("\n" + "="*80)
print("PARTE 3: CIERRE Y DISCUSI√ìN")
print("="*80)

cierre_discusion = """

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                     CIERRE Y DISCUSI√ìN - 30 MINUTOS                        ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

1. REPASO DE ACTIVIDADES REALIZADAS
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

‚úÖ Actividades completadas:
   ‚Ä¢ Configuraci√≥n de cl√∫ster EMR con HDFS
   ‚Ä¢ Conexi√≥n SSH al nodo maestro
   ‚Ä¢ Creaci√≥n de directorios en HDFS
   ‚Ä¢ Carga de archivos locales a HDFS
   ‚Ä¢ Lectura de datos con PySpark desde HDFS
   ‚Ä¢ Procesamiento y transformaci√≥n de datos
   ‚Ä¢ An√°lisis y agregaciones
   ‚Ä¢ Escritura de resultados en m√∫ltiples formatos
   ‚Ä¢ Verificaci√≥n de datos guardados


2. DIFICULTADES ENCONTRADAS
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

üî¥ DIFICULTADES T√âCNICAS COMUNES:

a) Conexi√≥n SSH al nodo maestro:
   Problema: Errores con claves SSH (.pem) y permisos
   Soluci√≥n: Usar AWS CloudShell como alternativa
   Comando CloudShell: 
   aws emr ssh --cluster-id j-XXXXXXXXXXXXX --key-pair-file ~/key.pem

b) Sintaxis de comandos HDFS:
   Problema: Confusi√≥n entre comandos Linux y HDFS
   Recordar: Siempre usar prefijo "hdfs dfs -" para operaciones en HDFS
   Ejemplo: hdfs dfs -ls (no solo "ls")

c) Rutas en HDFS:
   Problema: No distinguir rutas locales de rutas HDFS
   Local: /home/hadoop/archivo.csv
   HDFS: hdfs:///user/data/archivo.csv

d) Tiempo de inicio del cl√∫ster:
   Problema: Impaciencia durante los 10-15 minutos de arranque
   Tip: Utilizar este tiempo para revisar teor√≠a o preparar archivos

e) Gesti√≥n de recursos:
   Problema: Nodos peque√±os sin memoria suficiente
   Soluci√≥n: Usar tipos de instancia apropiados (m5.xlarge recomendado)

üî¥ DIFICULTADES CONCEPTUALES:

a) Distribuci√≥n de bloques:
   ‚Ä¢ Entender c√≥mo un archivo se divide en bloques de 128MB
   ‚Ä¢ Comprender que cada bloque se replica en 3 nodos diferentes
   ‚Ä¢ Visualizar c√≥mo se distribuye la informaci√≥n f√≠sicamente

b) Almacenamiento local vs distribuido:
   ‚Ä¢ Local: Todo en un disco, si falla se pierde todo
   ‚Ä¢ Distribuido: Datos replicados, tolerancia a fallos

c) Replicaci√≥n de datos:
   ‚Ä¢ Por qu√© HDFS replica 3 veces (consume 3x espacio)
   ‚Ä¢ Trade-off: Espacio vs Disponibilidad vs Rendimiento


3. ¬øC√ìMO HDFS Y PYSPARK AYUDAN EN EL PROCESAMIENTO DISTRIBUIDO?
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

üü¢ HDFS PROPORCIONA:

a) Almacenamiento Escalable:
   ‚Ä¢ Datos divididos en bloques de 128MB
   ‚Ä¢ Distribuci√≥n entre m√∫ltiples nodos
   ‚Ä¢ Capacidad: Terabytes ‚Üí Petabytes
   
   Ejemplo: 1TB de datos = 8,000 bloques distribuidos en el cl√∫ster

b) Tolerancia a Fallos:
   ‚Ä¢ 3 r√©plicas por bloque por defecto
   ‚Ä¢ Si falla un nodo, datos disponibles en otros 2
   ‚Ä¢ NameNode detecta fallos y replica autom√°ticamente
   
   Ejemplo: En cl√∫ster de 100 nodos, pueden fallar 66 nodos 
           y a√∫n as√≠ los datos est√°n disponibles

c) Localidad de Datos:
   ‚Ä¢ Procesamiento donde est√°n los datos
   ‚Ä¢ Reduce transferencia de red
   ‚Ä¢ "Mover c√≥digo a datos" vs "Mover datos a c√≥digo"
   
   Impacto: Reducci√≥n de latencia del 90% en operaciones masivas

üü¢ PYSPARK PROPORCIONA:

a) Procesamiento Paralelo:
   ‚Ä¢ Divide trabajo autom√°ticamente entre nodos
   ‚Ä¢ Procesamiento simult√°neo en m√∫ltiples m√°quinas
   
   Ejemplo: 10 nodos ‚Üí 10x m√°s r√°pido que 1 m√°quina
           An√°lisis que tarda 10 horas ‚Üí 1 hora

b) Lazy Evaluation (Evaluaci√≥n Perezosa):
   ‚Ä¢ Spark construye un plan de ejecuci√≥n √≥ptimo
   ‚Ä¢ No ejecuta hasta que es necesario
   ‚Ä¢ Optimiza operaciones combinadas
   
   Beneficio: Reducci√≥n hasta 50% en operaciones redundantes

c) Abstracci√≥n de Complejidad:
   ‚Ä¢ C√≥digo como si fuera una sola m√°quina
   ‚Ä¢ Spark maneja la distribuci√≥n autom√°ticamente
   ‚Ä¢ APIs de alto nivel (DataFrames, SQL)
   
   Ventaja: Productividad del desarrollador aumenta 3-5x

üü¢ EJEMPLO PR√ÅCTICO DEL BENEFICIO COMBINADO:

Escenario: Analizar 1TB de logs de servidores web

‚ùå SIN DISTRIBUCI√ìN (1 servidor potente):
   ‚Ä¢ Tiempo: ~10 horas
   ‚Ä¢ Si falla: Perder todo el trabajo
   ‚Ä¢ Costo: Servidor de alto rendimiento caro

‚úÖ CON HDFS + PYSPARK (10 nodos medianos):
   ‚Ä¢ Tiempo: ~1 hora (10x m√°s r√°pido)
   ‚Ä¢ Si falla 1-2 nodos: Trabajo contin√∫a
   ‚Ä¢ Costo: Nodos econ√≥micos, escalable bajo demanda


4. CASOS PR√ÅCTICOS EN LA INDUSTRIA
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

üíº CASO 1: AN√ÅLISIS DE LOGS (Netflix, Amazon)
   Problema: Procesar 50TB de logs diarios
   Soluci√≥n: HDFS almacena logs, Spark analiza patrones
   Resultado: Detectar errores en minutos vs horas
   Beneficio: Mejor experiencia de usuario, menos downtime

üíº CASO 2: DETECCI√ìN DE FRAUDE (Bancos, PayPal)
   Problema: Analizar 100M de transacciones/d√≠a en tiempo real
   Soluci√≥n: Spark Streaming + HDFS para hist√≥rico
   Resultado: Detecci√≥n de fraude en segundos
   Beneficio: Prevenci√≥n de p√©rdidas millonarias

üíº CASO 3: IoT Y SMART CITIES (Empresas municipales)
   Problema: 10,000 sensores generando datos cada segundo
   Soluci√≥n: HDFS para series temporales, Spark para an√°lisis
   Resultado: Optimizaci√≥n de tr√°fico y energ√≠a
   Beneficio: Ahorro 20-30% en costos operativos

üíº CASO 4: RECOMENDACIONES (Spotify, YouTube, Netflix)
   Problema: Procesar billones de interacciones usuario-contenido
   Soluci√≥n: HDFS guarda hist√≥rico, Spark entrena modelos ML
   Resultado: Recomendaciones personalizadas precisas
   Beneficio: +40% engagement, +25% retenci√≥n de usuarios

üíº CASO 5: AN√ÅLISIS SATELITAL (NASA, Agricultura)
   Problema: Procesar terabytes de im√°genes satelitales
   Soluci√≥n: HDFS almacena im√°genes, Spark procesa en paralelo
   Resultado: Detecci√≥n de cambios clim√°ticos y cultivos
   Beneficio: Predicciones agr√≠colas m√°s precisas

üíº CASO 6: REDES SOCIALES (Meta, Twitter)
   Problema: Procesar millones de posts/segundo
   Soluci√≥n: HDFS para grafo social, Spark para an√°lisis
   Resultado: Detecci√≥n de tendencias y moderaci√≥n autom√°tica
   Beneficio: Plataforma m√°s segura y relevante


5. COMPARACI√ìN: CON Y SIN TECNOLOG√çAS BIG DATA
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ M√âTRICA          ‚îÇ SIN BIG DATA      ‚îÇ CON HDFS + PYSPARK              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Tiempo proceso   ‚îÇ 10 horas          ‚îÇ 1 hora (10x m√°s r√°pido)         ‚îÇ
‚îÇ Escalabilidad    ‚îÇ Vertical (cara)   ‚îÇ Horizontal (econ√≥mica)          ‚îÇ
‚îÇ Tolerancia fallo ‚îÇ Ninguna           ‚îÇ Alta (r√©plicas)                 ‚îÇ
‚îÇ Capacidad        ‚îÇ Limitada (1 disco)‚îÇ Petabytes                       ‚îÇ
‚îÇ Costo            ‚îÇ $10,000/mes       ‚îÇ $2,000/mes (bajo demanda)       ‚îÇ
‚îÇ Flexibilidad     ‚îÇ Baja              ‚îÇ Alta (a√±adir nodos)             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò


6. PREGUNTAS PARA REFLEXI√ìN
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

‚ùì ¬øEn qu√© situaciones NO usar√≠as HDFS?
   ‚Üí Datos peque√±os (< 1GB)
   ‚Üí Aplicaciones transaccionales (OLTP)
   ‚Üí Necesidad de latencia ultra-baja (< 10ms)

‚ùì ¬øQu√© pasar√≠a si el NameNode falla?
   ‚Üí Sin HA: Cl√∫ster inoperativo
   ‚Üí Con HA: Secondary NameNode toma el control

‚ùì ¬øC√≥mo elegir el n√∫mero de r√©plicas?
   ‚Üí 3 r√©plicas: Balance est√°ndar
   ‚Üí 2 r√©plicas: Menos redundancia, m√°s espacio
   ‚Üí 4+ r√©plicas: Datos cr√≠ticos, m√°s seguridad

‚ùì ¬øCu√°ndo usar Parquet vs CSV?
   ‚Üí Parquet: An√°lisis frecuentes, queries selectivas
   ‚Üí CSV: Intercambio de datos, compatibilidad


7. RECURSOS PARA CONTINUAR APRENDIENDO
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

üìö Documentaci√≥n Oficial:
   ‚Ä¢ Apache Spark: https://spark.apache.org/docs/latest/
   ‚Ä¢ AWS EMR: https://docs.aws.amazon.com/emr/
   ‚Ä¢ HDFS: https://hadoop.apache.org/docs/

üìñ Libros Recomendados:
   ‚Ä¢ "Learning Spark" (O'Reilly)
   ‚Ä¢ "Hadoop: The Definitive Guide" (O'Reilly)
   ‚Ä¢ "High Performance Spark" (O'Reilly)

üéì Cursos Online:
   ‚Ä¢ Coursera: Big Data Specialization
   ‚Ä¢ DataCamp: PySpark courses
   ‚Ä¢ Udemy: Spark and Hadoop Developer

üíª Pr√°ctica con Datos Reales:
   ‚Ä¢ Kaggle: Big Data competitions
   ‚Ä¢ AWS Open Data: Datasets p√∫blicos
   ‚Ä¢ Google BigQuery: Public datasets


8. PR√ìXIMOS PASOS SUGERIDOS
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

üéØ Nivel Intermedio:
   ‚Ä¢ Integrar HDFS con Apache Hive para consultas SQL
   ‚Ä¢ Usar Spark MLlib para machine learning distribuido
   ‚Ä¢ Implementar particionamiento de datos en HDFS

üéØ Nivel Avanzado:
   ‚Ä¢ Configurar cl√∫ster en Alta Disponibilidad (HA)
   ‚Ä¢ Optimizar performance con cach√© y persistencia
   ‚Ä¢ Implementar Spark Streaming para datos en tiempo real
   ‚Ä¢ Integrar con Kafka para pipelines de datos

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                         ¬°PR√ÅCTICA COMPLETADA!                              ‚ïë
‚ïë                                                                            ‚ïë
‚ïë  Has aprendido los fundamentos de HDFS y PySpark en un entorno real.     ‚ïë
‚ïë  Estos conocimientos son la base para trabajar con Big Data.              ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
"""

print(cierre_discusion)

# ============================================================================
# LIMPIEZA Y CIERRE
# ============================================================================

print("\n" + "="*80)
print("LIMPIEZA Y CIERRE")
print("="*80)

limpieza = """
COMANDOS DE LIMPIEZA EN TERMINAL SSH:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

# Ver todos los archivos creados
hdfs dfs -ls -R /user/data/

# Eliminar archivos de salida
hdfs dfs -rm -r /user/data/output_*

# Eliminar estad√≠sticas
hdfs dfs -rm -r /user/data/city_statistics

# Limpiar todo el directorio (CUIDADO)
hdfs dfs -rm -r /user/data/*

# Verificar espacio liberado
hdfs dfs -du -h /user/

# Verificar estado del sistema
hdfs dfsadmin -report


PARA DETENER EL CL√öSTER EMR (Importante para no gastar cr√©ditos):
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

1. En la consola AWS EMR
2. Seleccionar el cl√∫ster
3. Clic en "Terminate"
4. Confirmar terminaci√≥n

‚ö†Ô∏è  NOTA: Terminar el cl√∫ster eliminar√° todos los datos en HDFS
    Si necesitas conservar datos, gu√°rdalos en S3 antes de terminar
"""

print(limpieza)

# Detener sesi√≥n de Spark
print("\nüìå Para detener la sesi√≥n de Spark, ejecuta:")
print("spark.stop()")

print("\n" + "="*80)
print("‚úÖ ¬°PR√ÅCTICA COMPLETADA EXITOSAMENTE!")
print("="*80)
print("""
Conceptos Aprendidos:
‚úì Arquitectura y funcionamiento de HDFS
‚úì Comandos b√°sicos para interactuar con HDFS
‚úì Lectura y escritura de datos con PySpark
‚úì Transformaciones y filtros de datos distribuidos
‚úì Agregaciones y an√°lisis estad√≠stico
‚úì Diferentes formatos de almacenamiento (CSV, Parquet)
‚úì Beneficios del procesamiento distribuido
‚úì Casos de uso reales en la industria

¬°Felicitaciones! Ahora tienes las bases para trabajar con Big Data.
""")
