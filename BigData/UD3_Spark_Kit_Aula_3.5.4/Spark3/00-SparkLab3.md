# Laboratorio 3 â€” Spark, Parquet y particionado
## UD3 Â· Procesamiento eficiente de datos

En este laboratorio vamos a dar el salto de **CSV â†’ Parquet**, y a entender por quÃ© en Big Data **el formato y la organizaciÃ³n de los datos importan tanto como el motor de procesamiento**.

---

## 1. Objetivos del laboratorio

Al finalizar este laboratorio deberÃ­as ser capaz de:

- Comprender por quÃ© CSV no es adecuado para Big Data.
- Trabajar con **Parquet**, un formato columnar.
- Aplicar **particionado** de datos con Spark.
- Comparar rendimiento entre:
  - CSV sin particionar
  - Parquet particionado
- Preparar datos para su consumo por herramientas de anÃ¡lisis y visualizaciÃ³n.

---

## 2. Punto de partida

Partimos del trabajo realizado en el Laboratorio 2:

- Dataset inflado (`ventas_clientes_anon_big.csv`)
- Procesamiento con Spark
- Resultados agregados por ciudad

Estructura esperada:

```

ud3-spark/
apps/
lab1_job.py
data/
ventas_clientes_anon_big.csv
output/

```

---

## 3. Problema: CSV en Big Data

Antes de hacer nada, reflexiona:

- CSV:
  - No tiene tipos explÃ­citos
  - No estÃ¡ comprimido eficientemente
  - Obliga a leer todo el fichero
- En Big Data:
  - Se leen **columnas concretas**
  - Se filtran **particiones**
  - Se accede a **fragmentos del dataset**

ğŸ‘‰ AquÃ­ es donde entra **Parquet**.

---

## 4. Escritura en formato Parquet

### 4.1 Crear un nuevo job Spark

Copia `lab1_job.py` y crea:

```

apps/lab3_parquet_job.py

````

---

### 4.2 CÃ³digo base del job

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

DATA_PATH = "/opt/spark-data/ventas_clientes_anon_big.csv"
OUTPUT_PATH = "/opt/spark-data/parquet/ventas"

spark = SparkSession.builder.appName("UD3-Lab3-Parquet").getOrCreate()

df = spark.read.csv(DATA_PATH, header=True, inferSchema=True)

# Escritura en Parquet
df.write.mode("overwrite").parquet(OUTPUT_PATH)

spark.stop()
```

---

### 4.3 Ejecutar el job

Desde el Master:

```bash
docker exec -it spark-master /opt/spark/bin/spark-submit \
  --master spark://IP_DEL_MASTER:7077 \
  /opt/spark-apps/lab3_parquet_job.py
```

Comprueba que se ha creado:

```
data/parquet/ventas/
```

Con mÃºltiples ficheros `.parquet`.

---

## 5. Lectura desde Parquet

### 5.1 Modificar el job para leer Parquet

Edita el mismo fichero:

```python
df = spark.read.parquet(OUTPUT_PATH)
```

Y aÃ±ade una operaciÃ³n simple:

```python
df.filter(col("importe") > 100).groupBy("ciudad").count().show()
```

Ejecuta de nuevo el job.

---

## 6. Particionado de datos

### 6.1 Â¿QuÃ© es particionar?

Particionar significa **organizar los datos en carpetas segÃºn una columna**.

Ejemplo:

```
ciudad=Cadiz/
ciudad=Jerez/
ciudad=Algeciras/
```

Esto permite:

* leer solo una parte del dataset
* acelerar consultas
* reducir E/S

---

### 6.2 Escritura en Parquet particionado

Modifica la escritura:

```python
df.write \
  .mode("overwrite") \
  .partitionBy("ciudad") \
  .parquet(OUTPUT_PATH)
```

Ejecuta el job de nuevo.

---

### 6.3 Inspeccionar la estructura

Explora la carpeta:

```
data/parquet/ventas/
```

Observa:

* subcarpetas por ciudad
* ficheros `.parquet` dentro

---

## 7. ComparaciÃ³n de rendimiento (conceptual)

No hace falta medir milisegundos exactos.

Reflexiona sobre:

* CSV:

  * lee todo el fichero
* Parquet:

  * lee solo columnas necesarias
* Parquet particionado:

  * lee solo las carpetas relevantes

Ejemplo mental:

> â€œQuiero solo ventas de CÃ¡dizâ€

* CSV â†’ leer todo
* Parquet â†’ leer columnas
* Parquet + particiÃ³n â†’ leer solo CÃ¡diz

---

## 8. PreparaciÃ³n para anÃ¡lisis y visualizaciÃ³n

El resultado en Parquet:

* es estable
* es eficiente
* estÃ¡ listo para:

  * Kibana
  * Zeppelin
  * procesos periÃ³dicos
  * pipelines automÃ¡ticos

ğŸ‘‰ Esto es **lo que se hace en sistemas reales**.

---

## 9. Preguntas de reflexiÃ³n (para la entrega)

Responde en el documento de entrega:

1. Â¿Por quÃ© CSV no es adecuado para Big Data?
2. Â¿QuÃ© ventaja principal aporta Parquet?
3. Â¿QuÃ© problema resuelve el particionado?
4. Â¿CuÃ¡ndo NO tendrÃ­a sentido particionar?
5. Â¿QuÃ© formato usarÃ­as para un datalake?

---

## 10. QuÃ© deberÃ­as tener claro al terminar

Al finalizar este laboratorio deberÃ­as entender que:

* Spark no solo procesa datos, **organiza datos**.
* El formato de almacenamiento es clave.
* Parquet + particiones es un estÃ¡ndar de facto.
* Esto conecta directamente con:

  * datalakes
  * BI
  * ML
  * pipelines de datos

---

## 11. Lo que viene despuÃ©s

A partir de aquÃ­:

* VisualizaciÃ³n (Kibana)
* AnÃ¡lisis interactivo (Zeppelin)
* AutomatizaciÃ³n (Airflow)

ğŸ‘‰ **Este laboratorio es el puente** entre procesamiento y explotaciÃ³n.

