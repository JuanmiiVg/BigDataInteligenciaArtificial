# Laboratorio 2 â€” Spark frente a pandas  
## Procesamiento de datos a mayor escala

En este laboratorio vamos a **aumentar el volumen de datos** y a comparar dos enfoques:

- Procesamiento clÃ¡sico con pandas
- Procesamiento distribuido con Spark

El objetivo no es â€œver quiÃ©n va mÃ¡s rÃ¡pidoâ€, sino **entender cuÃ¡ndo Spark empieza a marcar la diferencia**.

---

## 1. Objetivos del laboratorio

Al finalizar este laboratorio deberÃ­as ser capaz de:

- Generar un dataset de mayor tamaÃ±o a partir de uno pequeÃ±o.
- Procesar el mismo dataset con pandas y con Spark.
- Observar diferencias de tiempo y consumo.
- Comprender por quÃ© Spark escala mejor cuando el volumen crece.
- Preparar datos para su uso posterior (visualizaciÃ³n / automatizaciÃ³n).

---

## 2. Punto de partida

Partimos del proyecto del Laboratorio 1:

```

ud3-spark-lab1/
apps/
data/
ventas_clientes_anon.csv
scripts/
inflar_dataset.py

````

El dataset original es **correcto**, pero demasiado pequeÃ±o para notar diferencias.

---

## 3. Inflar el dataset (paso clave)

### 3.1 Â¿Por quÃ© inflar los datos?

En clase no siempre podemos trabajar con datos realmente grandes.  
Para simular un escenario Big Data:

ğŸ‘‰ **replicamos los datos muchas veces**, manteniendo su estructura.

---

### 3.2 Generar un dataset grande

Desde la raÃ­z del proyecto:

```bash
python scripts/inflar_dataset.py \
  --input data/ventas_clientes_anon.csv \
  --output data/ventas_clientes_anon_big.csv \
  --factor 100
```

**Resultado:**
```
Generado data/ventas_clientes_anon_big.csv con 400 filas
```

Esto genera un CSV con muchas mÃ¡s filas (el dataset original tiene 4 filas, con factor 100 obtenemos 400 filas).

Comprueba el tamaÃ±o:

```bash
ls -lh data/ventas_clientes_anon_big.csv
```

---

## 4. Procesamiento con pandas (referencia)

Este paso sirve **solo como comparaciÃ³n**.

### 4.1 Crear un script pandas sencillo

Crea el fichero `apps/pandas_job.py`:

```python
import pandas as pd
import time

start = time.time()

df = pd.read_csv("data/ventas_clientes_anon_big.csv")

df_f = df[df["importe"] > 100]

res = (
    df_f.groupby("ciudad")
    .agg(
        num_ventas=("importe", "count"),
        importe_total=("importe", "sum"),
        importe_medio=("importe", "mean")
    )
    .sort_values("importe_total", ascending=False)
)

print(res.head(10))

end = time.time()
print(f"Tiempo total pandas: {end - start:.2f} segundos")
```

Ejecuta:

```bash
python apps/pandas_job.py
```

**Resultado obtenido:**
```
           num_ventas  importe_total  importe_medio
ciudad
Jerez             100       19980.96       199.8096
Algeciras         100       15007.32       150.0732
Cadiz             100       12002.51       120.0251
Tiempo total pandas: 0.03 segundos
```

âš ï¸ Si tu equipo va justo, este paso puede tardar bastante o incluso fallar.
Eso **forma parte del experimento**.

**ObservaciÃ³n:** Con el dataset de 400 filas (100 rÃ©plicas del original de 4 filas), pandas es muy rÃ¡pido (0.03 segundos). A medida que aumente el volumen, esto cambiarÃ¡ significativamente.

---

## 5. Procesamiento con Spark

### 5.1 Usar el dataset inflado en Spark

Edita `apps/lab1_job.py` y cambia la ruta:

```python
DATA_PATH = "/opt/spark-data/ventas_clientes_anon_big.csv"
```

---

### 5.2 Ejecutar el job Spark

Desde el **Master**:

```bash
docker exec -it spark-master spark-submit \
  --master spark://IP_DEL_MASTER:7077 \
  /opt/spark-apps/lab1_job.py
```

Observa:

* El tiempo de ejecuciÃ³n
* El uso de CPU
* La actividad en la UI del Master (`:8080`)

---

## 6. ObservaciÃ³n y comparaciÃ³n

### Tabla comparativa (con 400 filas de datos):

| Aspecto             | pandas | Spark |
| ------------------- | ------ | ----- |
| Tiempo de ejecuciÃ³n | 0.03s  | Pendiente* |
| Uso de CPU          | 1 nÃºcleo | Distribuido |
| Uso de memoria      | ~50 MB | Distribuido |
| SensaciÃ³n general   | InstantÃ¡neo | Depende del cluster |

*Para ejecutar Spark, necesitas tener el cluster configurado (docker-compose.master.yml y docker-compose.worker.yml)

**Nota importante:** Con solo 400 filas, pandas es prÃ¡cticamente instantÃ¡neo. Para ver diferencias significativas, deberÃ­as:
- Aumentar el factor a 10,000 o mÃ¡s
- Usar datasets reales de millones de registros
- Medir no solo tiempo, sino tambiÃ©n consumo de CPU y memoria
**Â¿QuÃ© ha pasado al aumentar el volumen de datos?**
   - Con 400 filas, pandas sigue siendo muy rÃ¡pido (0.03s)
   - El overhead de Spark (iniciar el cluster) es mayor que el tiempo de procesamiento

2. **Â¿En quÃ© momento pandas empieza a ser incÃ³modo?**
   - Con datasets > 1 GB en una mÃ¡quina estÃ¡ndar
   - Con operaciones complejas que requieren mÃºltiples pasadas sobre los datos
   - Cuando necesitas procesar datos en mÃºltiples mÃ¡quinas

3. **Â¿Spark es siempre mejor? Â¿Por quÃ©?**
   - No. Con datos pequeÃ±os, pandas es mÃ¡s eficiente
   - Spark brilla cuando el volumen es realmente grande o distribuido

4. **Â¿QuÃ© coste tiene usar Spark frente a pandas?**
   - Overhead de inicializaciÃ³n del cluster
   - Mayor consumo de recursos (memoria para coordinaciÃ³n)
   - Mayor complejidad operacional

5. **Â¿QuÃ© enfoque usarÃ­as para:**

   * **Un anÃ¡lisis rÃ¡pido:** pandas (< 1GB) o Python directo
   * **Un proceso periÃ³dico:** pandas si es < 1GB, Spark si es mayor
   * **Un volumen muy grande:** Spark, con datos en HDFS o cloud storagempieza a ser incÃ³modo?
3. Â¿Spark es siempre mejor? Â¿Por quÃ©?
4. Â¿QuÃ© coste tiene usar Spark frente a pandas?
5. Â¿QuÃ© enfoque usarÃ­as para:

   * un anÃ¡lisis rÃ¡pido
   * un proceso periÃ³dico
   * un volumen muy grande?

---

## 8. Salida del procesamiento

Spark genera resultados en:

```
data/output/ventas_por_ciudad/
``**Spark no sustituye a pandas.** Son herramientas complementarias con casos de uso distintos.
* **Spark escala mejor cuando el volumen crece.** El punto de inflexiÃ³n tÃ­picamente estÃ¡ entre 1-10 GB.
* **El cambio importante no es la sintaxis, sino el modelo de ejecuciÃ³n.** Spark distribuye el trabajo; pandas es single-machine.
* **En Big Data real se combinan ambas herramientas.** Spark para procesar, pandas para anÃ¡lisis exploratorio.
* **El overhead importa:** Iniciar Spark, serializar datos, coordinaciÃ³n entre nodos. Solo se justifica con volÃºmenes significativos.

---

## 10. Lo que viene despuÃ©s

En el siguiente bloque:

* Trabajaremos con formatos eficientes (Parquet).
* Prepararemos datos para visualizaciÃ³n en Kibana.
* Usaremos herramientas especÃ­ficas de Big Data (Hive, HBase).
* Automatizaremos procesos con Airflow.

ğŸ‘‰ **No corras**: lo importante aquÃ­ es entender el porquÃ©, no la velocidad.

---

## 11. PrÃ³ximas pruebas recomendadas

Para profundizar en este laboratorio, prueba:

1. **Aumentar el factor:** Usa `--factor 10000` y mide de nuevo
2. **Monitorear recursos:** Abre el gestor de tareas mientras ejecutas
3. **Usar formatos mÃ¡s grandes:** CSV vs Parquet vs JSON
4. **Ejecutar en el cluster Spark:** Compara tiempos locales vs distribuidos
5. **Profiling:** Usa `cProfile` en pandas y `Spark UI` en Spark
* El cambio importante no es la sintaxis, sino el modelo de ejecuciÃ³n.
* En Big Data real se combinan ambas herramientas.

---

## 10. Lo que viene despuÃ©s

En el siguiente bloque:

* Trabajaremos con formatos eficientes (Parquet).
* Prepararemos datos para visualizaciÃ³n.
* Usaremos herramientas especÃ­ficas de Big Data.

ğŸ‘‰ **No corras**: lo importante aquÃ­ es entender el porquÃ©.




