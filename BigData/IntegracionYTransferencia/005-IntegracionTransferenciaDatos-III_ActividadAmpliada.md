
---

# üß™ **2. Actividad Detallada (Versi√≥n ampliada)**

(dise√±ada para UD2 o para el compa√±ero de BD aplicado)

Te la dejo en formato did√°ctico, paso a paso y aut√≥noma.
Lista para entregar al alumnado o integrarla en el aula virtual.

---

# üéì **Actividad: Construcci√≥n de un Pipeline Moderno de Ingesta e Integraci√≥n de Datos (Airbyte + CDC + Kafka + Python)**

## ‚úîÔ∏è Objetivo general

Construir un pipeline real completo utilizando:

* **PostgreSQL Cloud** (Neon)
* **Airbyte Cloud** (ingesta batch/incremental)
* **Confluent Cloud (Kafka)** (CDC)
* **Python (Google Colab)** (consumer streaming)

El alumnado experimentar√° **dos enfoques modernos**:

* *Batch/Incremental ELT* (Airbyte)
* *CDC Streaming* (Debezium-like con Confluent)

El ejercicio no requiere instalaci√≥n local de nada.

---

## üß© **1. Crear la base de datos en Neon (PostgreSQL Cloud)**

1. Entrar en [https://neon.tech](https://neon.tech)
2. Crear un proyecto nuevo
3. Anotar los datos:

   * HOST
   * USER
   * PASSWORD
   * DATABASE
   * PORT
4. En el SQL Editor crear la tabla:

```sql
CREATE TABLE clientes (
  id SERIAL PRIMARY KEY,
  nombre VARCHAR(100),
  email VARCHAR(100),
  actualizado TIMESTAMP DEFAULT NOW()
);
```

5. Insertar datos:

```sql
INSERT INTO clientes(nombre, email)
VALUES ('Ana', 'ana@example.com'),
       ('Luis', 'luis@example.com');
```

---

## üü¶ **2. Crear el CDC con Confluent Cloud (Kafka gestionado)**

1. Entrar en [https://confluent.cloud](https://confluent.cloud)
2. Crear cuenta gratuita
3. Crear cluster ‚ÄúBasic‚Äù
4. Ir a **Connectors ‚Üí PostgreSQL CDC Source**
5. Configurar:

   * Host = host de Neon
   * User/password
   * Database
   * Table inclusion: `clientes`
6. Seleccionar topic destino:

   ```
   clientes_cdc
   ```

Una vez activado, cualquier cambio en la tabla `clientes` aparecer√° como evento CDC en Kafka.

---

## üü® **3. Ingesta batch/incremental con Airbyte Cloud**

1. Entrar en [https://cloud.airbyte.com](https://cloud.airbyte.com)
2. Crear cuenta gratuita
3. Crear **Source**:

   * Tipo: PostgreSQL
   * Host/DB/User/Password = datos de Neon
4. Crear **Destination**:

   * Tipo: *File (JSON o Parquet)*
   * Carpeta interna: `airbyte_output/`
5. Crear **Connection**:

   * Sync mode: *Incremental*
   * Cursor: `actualizado`
   * Primary key: `id`
6. Ejecutar sincronizaci√≥n manual

El resultado ser√° un conjunto de ficheros Parquet/JSON exportados.

---

## üêç **4. Crear el consumidor en Google Colab**

1. Abrir [https://colab.research.google.com](https://colab.research.google.com)
2. Instalar librer√≠a:

```python
!pip install confluent-kafka
```

3. A√±adir c√≥digo del consumidor:

```python
from confluent_kafka import Consumer
import json

conf = {
    'bootstrap.servers': 'CLUSTER_BOOTSTRAP_URL',
    'security.protocol': 'SASL_SSL',
    'sasl.mechanisms': 'PLAIN',
    'sasl.username': 'API_KEY',
    'sasl.password': 'API_SECRET',
    'group.id': 'grupo1',
    'auto.offset.reset': 'earliest'
}

consumer = Consumer(conf)
consumer.subscribe(["clientes_cdc"])

print("Esperando cambios CDC...\n")

while True:
    msg = consumer.poll(1.0)
    if msg is None:
        continue
    if msg.error():
        print("Error:", msg.error())
        continue

    evento = json.loads(msg.value().decode("utf-8"))
    print(json.dumps(evento, indent=2))
```

4. Ejecutar el notebook
5. Volver a Neon y modificar datos:

```sql
UPDATE clientes 
SET nombre = 'Ana G√≥mez' 
WHERE id = 1;
```

En Colab aparece inmediatamente el evento CDC.

---

## üìÑ **5. Documentaci√≥n a entregar**

El alumnado debe entregar:

* Diagrama del pipeline (simple o con Mermaid)
* Capturas de:

  * Airbyte Source
  * Airbyte Destination
  * Airbyte Sync
  * Conector CDC en Confluent
  * Topic con mensajes
  * Colab mostrando eventos
* Script del consumidor explicando cada l√≠nea
* Informe t√©cnico (1‚Äì2 p√°ginas):

  * Qu√© herramientas han usado
  * Qu√© problema resuelve cada una
  * Ventajas/inconvenientes
  * Qu√© podr√≠an a√±adir o mejorar

---

## üéØ **6. Resultado esperado**

Al final, los alumnos habr√°n construido un pipeline que:

* lee datos batch/incrementales desde PostgreSQL ‚Üí Airbyte
* captura cambios CDC PostgreSQL ‚Üí Confluent Kafka
* los visualiza en tiempo real ‚Üí Python (Colab)

---

# üëç **Todo listo.**


