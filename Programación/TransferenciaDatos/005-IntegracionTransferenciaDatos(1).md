# Herramientas modernas de integraci√≥n y transferencia de datos: Airbyte, Debezium y Kafka



# üìò **Introducci√≥n a la integraci√≥n y transferencia de datos en sistemas Big Data**

En cualquier sistema de Big Data, los datos no se encuentran de forma natural dentro del *data lake* o del sistema anal√≠tico: deben **llegar desde m√∫ltiples or√≠genes**, a menudo heterog√©neos, distribuidos y con distintas velocidades de generaci√≥n.
A este proceso global lo llamamos **integraci√≥n y transferencia de datos**, y constituye una de las fases m√°s cr√≠ticas de toda arquitectura Big Data.

En este contexto, la ingesta y la integraci√≥n son mucho m√°s que mover ficheros:
implican garantizar que los datos que llegan al sistema son **completos, consistentes, actualizados y utilizables**.

---

## üß© 1. ¬øQu√© significa *integrar* datos?

La **integraci√≥n de datos** es el proceso mediante el cual combinamos, unificamos o reconciliamos datos procedentes de varias fuentes con el objetivo de obtener una visi√≥n coherente y homog√©nea.

Las fuentes pueden incluir:

* bases de datos transaccionales (OLTP)
* APIs REST
* archivos CSV, JSON, Parquet
* aplicaciones SaaS (CRM, ERP, anal√≠tica web‚Ä¶)
* sensores IoT o logs de sistemas

Integrar datos no significa √∫nicamente copiarlos:
implica **alinear sus estructuras, formatos, significados y tiempos**, y prepararlos para an√°lisis posterior.

### Objetivos clave de la integraci√≥n:

* Unificar informaci√≥n fragmentada.
* Convertir datos heterog√©neos en un formato com√∫n.
* Resolver inconsistencias y duplicados.
* Facilitar an√°lisis posteriores (Spark, Pandas, ML‚Ä¶).
* Mantener una versi√≥n actualizada y coherente de los datos.

### Ejemplo:

Un sistema de ventas puede tener:

* datos transaccionales en PostgreSQL,
* cat√°logos de productos en un SaaS externo,
* anal√≠tica web en una API,
* datos de clientes en otra base de datos.

La integraci√≥n permite unificarlos en una sola visi√≥n o modelo anal√≠tico.

---

## üîÑ 2. ¬øQu√© significa *transferir* datos?

La **transferencia de datos** consiste en mover los datos desde su ubicaci√≥n original hasta un lugar donde puedan almacenarse y procesarse (normalmente un *data lake*, un *warehouse* o un motor de streaming).

Esta transferencia puede adoptar varias modalidades:

### **A) Batch**

* Los datos se copian por lotes (cada noche, cada hora‚Ä¶).
* Adecuado cuando el tiempo real no es necesario.
* Ejemplo: exportar cada d√≠a la tabla de ventas.

### **B) Incremental**

* Solo se transfieren los registros nuevos o modificados desde la √∫ltima sincronizaci√≥n.
* Reduce costes y volumen de datos.
* Ejemplo: columnas `updated_at` o `created_at`.

### **C) CDC (Change Data Capture)**

* Se capturan los cambios en tiempo real desde los logs internos de la BD.
* Refleja **insert, update, delete** sin recargar la tabla completa.
* Ideal para sincronizaci√≥n continua.

### **D) Streaming**

* Flujo constante de datos: logs, m√©tricas, eventos, IoT, clics‚Ä¶
* Requiere una plataforma capaz de manejar grandes vol√∫menes en tiempo real.

---

## üåê 3. ¬øC√≥mo encajan la integraci√≥n y la transferencia en una arquitectura Big Data?

En un sistema moderno de Big Data, el flujo general suele ser:

```
      Origen de datos
 (BD, API, SaaS, ficheros, IoT)
             ‚îÇ
             ‚ñº
     Transferencia / Ingesta
 (batch, incremental, CDC, streaming)
             ‚îÇ
             ‚ñº
       Data Lake / S3
   (raw ‚Üí staged ‚Üí curated)
             ‚îÇ
             ‚ñº
       Procesamiento
 (Spark, Flink, Pandas, ML)
             ‚îÇ
             ‚ñº
        Consumo anal√≠tico
    (dashboards, modelos, apps)
```

La **integraci√≥n** garantiza la coherencia de los datos.
La **transferencia** garantiza que los datos llegan al lugar adecuado en el momento adecuado.

Ambas conforman el **pipeline de datos** que es la base de toda arquitectura Big Data moderna.

---

## ‚öôÔ∏è 4. ¬øQu√© retos existen en la integraci√≥n y transferencia?

1. **Heterogeneidad de formatos**

   * SQL, CSV, JSON, APIs REST, logs, binarios‚Ä¶

2. **Velocidades distintas**

   * Algunos or√≠genes generan datos a diario, otros cada segundo.

3. **Volumen elevado**

   * Transferir millones de filas requiere herramientas escalables.

4. **Consistencia y calidad**

   * Evitar duplicados, inconsistencias temporales o registros corruptos.

5. **Trazabilidad y auditor√≠a**

   * Es fundamental saber *cu√°ndo* y *c√≥mo* se movieron los datos.

6. **Seguridad y privacidad**

   * Especialmente relevante en datos personales (RGPD).

---

## üí° 5. ¬øPor qu√© necesitamos herramientas espec√≠ficas?

Si bien en los primeros a√±os del Big Data se usaban herramientas propias del ecosistema Hadoop (Sqoop, Flume), el panorama tecnol√≥gico ha evolucionado. Hoy se utilizan plataformas m√°s modernas que:

* funcionan tanto en on-premise como en cloud,
* permiten conectores declarativos,
* soportan CDC y streaming,
* son f√°ciles de monitorizar y operar,
* siguen patrones ELT modernos.

Por eso trabajamos con:

* **Airbyte** para ingesta batch e incremental,
* **Debezium/CDC (v√≠a Confluent)** para cambios en tiempo real,
* **Kafka** para flujos de eventos y streaming.

---
Perfecto.
Aqu√≠ tienes **tres diagramas Mermaid** que puedes usar en la secci√≥n "Integraci√≥n y transferencia de datos".
Puedes usar uno solo o varios seg√∫n d√≥nde quieras insertarlos.

Incluyo:

1. **Diagrama general del flujo de integraci√≥n y transferencia**
2. **Diagrama comparativo de modos de transferencia (batch / incremental / CDC / streaming)**
3. **Diagrama completo del pipeline moderno (Airbyte + CDC + Kafka + Data Lake)**

Todos en sintaxis lista para copiar en Markdown.

---

# üé® **1. Diagrama general ‚Äî Integraci√≥n y transferencia de datos**

```mermaid
flowchart LR
    A[Fuentes de datos<br/>(BD, APIs, SaaS, CSV, IoT)] --> B[Ingesta y transferencia<br/>(Batch, Incremental, CDC, Streaming)]
    B --> C[Data Lake / Almacenamiento<br/>(RAW / STAGED / CURATED)]
    C --> D[Procesamiento<br/>(Spark, Flink, Pandas, ML)]
    D --> E[Consumo anal√≠tico<br/>(Dashboards, Informes, Modelos)]
```

---

# üîÑ **2. Modos de transferencia ‚Äî Diagrama comparativo**

```mermaid
flowchart TB
    A[Transferencia de datos] --> B1[Batch<br/>Procesamiento por lotes]
    A --> B2[Incremental<br/>Solo datos nuevos o modificados]
    A --> B3[CDC<br/>Captura de cambios en tiempo real]
    A --> B4[Streaming<br/>Flujo continuo de eventos]

    B1 --> C1[Ejemplo: copiar tabla diaria]
    B2 --> C2[Ejemplo: usar columna updated_at]
    B3 --> C3[Ejemplo: Debezium / Confluent CDC]
    B4 --> C4[Ejemplo: Kafka / IoT / logs]
```

---

# üß© **3. Pipeline moderno completo ‚Äî Airbyte + CDC + Kafka + Data Lake**

Este es perfecto para situar a los alumnos antes de la pr√°ctica.

```mermaid
flowchart LR

    %% Fuentes
    A1[(PostgreSQL<br/>Transaccional)]
    A2[(APIs / SaaS)]
    A3[(Ficheros CSV/JSON)]

    %% Airbyte (batch/incremental)
    A1 --> B1[Airbyte<br/>Ingesta ELT]
    A2 --> B1
    A3 --> B1

    %% Data Lake
    B1 --> C1[(Data Lake<br/>S3 / MinIO / Parquet)]

    %% CDC Pipeline
    A1 --> B2[Confluent Cloud<br/>CDC Source]
    B2 --> D1[(Kafka Topic<br/>clientes_cdc)]

    %% Consumer
    D1 --> E1[Python Consumer<br/>Google Colab]

    %% Procesamiento
    C1 --> F1[Spark / Flink / Pandas<br/>Procesamiento batch]
    D1 --> F2[Spark Streaming / ML en tiempo real]

    %% Consumo
    F1 --> G[Dashboards / BI / Reporting]
    F2 --> G
```

---

# ‚úîÔ∏è ¬øQuieres que lo integre directamente en la secci√≥n de teor√≠a dentro del documento 005?


## 6. Introducci√≥n a las herramientas de integraci√≥n de datos moderna

En los primeros sistemas Big Data basados en Hadoop eran habituales herramientas como **Sqoop** y **Flume** para mover datos entre bases de datos relacionales, HDFS y sistemas de logs. Sin embargo, estos proyectos han quedado pr√°cticamente **obsoletos** y han sido sustituidos por soluciones m√°s flexibles, pensadas para la nube y para arquitecturas *streaming*.

En la pr√°ctica actual, la integraci√≥n de datos se apoya en tres grandes piezas:

- **Airbyte**: integraci√≥n de datos (*ELT*) entre or√≠genes (APIs, ficheros, bases de datos, SaaS) y destinos (data lakes, data warehouses, bases anal√≠ticas).
- **Debezium**: captura de cambios en bases de datos relacionales (**CDC, Change Data Capture**) en tiempo (casi) real.
- **Kafka + Kafka Connect**: plataforma de *streaming* y eventos que permite recibir, distribuir y persistir flujos de datos de alta velocidad.

El objetivo de esta secci√≥n es entender **qu√© hace cada herramienta**, **en qu√© casos usarla** y ver **ejemplos simples** que puedan servir de base para las pr√°cticas de la unidad.

---

## 7. Airbyte: integraci√≥n ELT sin c√≥digo

### 7.1. ¬øQu√© es Airbyte?

**Airbyte** es una plataforma de integraci√≥n de datos (*data integration*) que permite mover datos desde muy diversos or√≠genes hasta un destino, utilizando conectores ya preparados. Se puede usar en dos modalidades:

- **Airbyte OSS**: versi√≥n *open-source* que se despliega en local o en un servidor (por ejemplo, con Docker).
- **Airbyte Cloud**: servicio gestionado en la nube, donde el proveedor se encarga de la infraestructura.

En ambos casos, la idea es la misma:

> Configurar *sources* (or√≠genes) y *destinations* (destinos), definir qu√© datos se quieren copiar y con qu√© frecuencia, y dejar que Airbyte se encargue del resto.

### 7.2. Componentes b√°sicos

- **Source (origen)**: de d√≥nde vienen los datos. Ej.: PostgreSQL, MySQL, API REST, fichero CSV en S3, Google Sheets‚Ä¶
- **Destination (destino)**: d√≥nde se escriben los datos. Ej.: MinIO/S3, BigQuery, Snowflake, PostgreSQL, data warehouse‚Ä¶
- **Connection (conexi√≥n)**: une un source con un destino y define:
  - qu√© tablas/streams se copian,
  - el modo de sincronizaci√≥n (completo, incremental),
  - la frecuencia (manual, cada X horas, cron‚Ä¶).

### 7.3. Caso de uso t√≠pico

Imaginemos un sistema transaccional en PostgreSQL con una tabla `ventas`. Queremos copiarla peri√≥dicamente a un *data lake* en formato Parquet:

- **Source**: conector PostgreSQL.
- **Destination**: conector S3/MinIO en modo Parquet.
- **Connection**: sincronizaci√≥n incremental diaria.

### 2.4. Ejemplo de configuraci√≥n simplificada

Airbyte se configura normalmente desde la interfaz web, pero conceptualmente podr√≠amos representarlo as√≠:

```json
{
  "source": {
    "type": "postgres",
    "host": "db.internal",
    "port": 5432,
    "database": "ventas_db",
    "username": "airbyte",
    "password": "********"
  },
  "destination": {
    "type": "s3",
    "endpoint": "https://minio.ejemplo.local",
    "bucket_name": "datalake",
    "path_prefix": "raw/ventas/",
    "format": "parquet"
  },
  "sync": {
    "mode": "incremental",
    "cursor_field": "updated_at",
    "primary_key": ["id"],
    "schedule": "0 2 * * *"
  }
}
```

> No es necesario escribir JSON: usaremos  la interfaz web, pero este ejemplo ayuda a entender qu√© par√°metros hay ¬´debajo¬ª.

---

## 3. Debezium: captura de cambios en bases de datos (CDC)

### 3.1. ¬øQu√© es CDC?

Cuando trabajamos con bases de datos relacionales en producci√≥n (ERP, CRM, aplicaciones web, etc.), no basta con copiar la tabla completa de vez en cuando. Queremos:

* saber **qu√© filas se han insertado**,
* **qu√© filas se han actualizado**,
* **qu√© filas se han borrado**,

y hacerlo en tiempo (casi) real, sin lanzar consultas pesadas o bloquear la base de datos.

A esto se le llama **Change Data Capture (CDC)**.

### 3.2. ¬øQu√© hace Debezium?

**Debezium** es una plataforma *open-source* dise√±ada para CDC. En lugar de leer las tablas directamente, se conecta a los **logs internos** de la base de datos (binlog, WAL, etc.) y convierte cada cambio en un evento:

* inserci√≥n ‚Üí evento ¬´create¬ª
* actualizaci√≥n ‚Üí evento ¬´update¬ª
* borrado ‚Üí evento ¬´delete¬ª

Normalmente Debezium se despliega junto a **Kafka Connect**, de forma que:

1. La base de datos genera sus logs internos de siempre (no hace falta modificar la aplicaci√≥n).
2. Debezium lee esos logs y los env√≠a a Kafka.
3. Cada tabla monitorizada se convierte en un *topic* de Kafka donde van llegando los cambios.

### 3.3. Ejemplo conceptual

Supongamos una tabla `pedidos` en PostgreSQL:

```sql
CREATE TABLE pedidos (
  id          SERIAL PRIMARY KEY,
  cliente_id  INTEGER NOT NULL,
  fecha       TIMESTAMP NOT NULL DEFAULT NOW(),
  total       NUMERIC(10,2) NOT NULL
);
```

Con Debezium configurado sobre esta base de datos:

* Cuando se inserta un nuevo pedido, aparece un mensaje en el *topic* `dbserver1.public.pedidos`.
* Cuando se actualiza un pedido (por ejemplo, cambia el `total`), se publica otro mensaje con **antes y despu√©s** de la fila.
* Lo mismo para las eliminaciones.

Estos mensajes pueden consumirse despu√©s desde Spark, Flink u otra herramienta para actualizar vistas anal√≠ticas, dashboards, etc.

Un ejemplo simplificado de mensaje (en formato JSON abreviado) podr√≠a ser:

```json
{
  "op": "u",
  "before": {"id": 10, "total": 50.00},
  "after":  {"id": 10, "total": 60.00},
  "ts_ms": 1731500000000
}
```

---

## 4. Kafka y Kafka Connect: *streaming* de eventos

### 4.1. Kafka como ¬´bus de datos¬ª en tiempo real

**Apache Kafka** es una plataforma de *streaming* distribuida que permite:

* Recibir datos (*producers*).
* Almacenarlos de forma distribuida (en *topics*).
* Servirlos a m√∫ltiples consumidores (*consumers*) en paralelo.

Se utiliza para:

* Logs de aplicaciones,
* eventos de usuarios (clics, navegaci√≥n),
* m√©tricas de sistemas,
* integraciones entre microservicios,
* y como ¬´tuber√≠a¬ª central donde Debezium y otras herramientas env√≠an sus datos.

### 4.2. Conceptos b√°sicos

* **Topic**: ¬´canal¬ª donde se publican mensajes (por ejemplo, `ventas`, `logs_servidor`).
* **Partition**: subdivisi√≥n de un topic para paralelizar la lectura/escritura.
* **Producer**: servicio que env√≠a mensajes a un topic.
* **Consumer**: servicio que lee mensajes de un topic.
* **Broker**: servidor Kafka que almacena y sirve los datos.

### 4.3. Kafka Connect

**Kafka Connect** es un componente de Kafka que simplifica la tarea de:

* leer datos de una fuente externa ‚Üí topic Kafka (conectores *source*),
* enviar datos desde un topic Kafka ‚Üí sistema externo (conectores *sink*).

Ejemplos de conectores *sink* habituales:

* sink a S3 / MinIO (para *data lake*),
* sink a Elasticsearch / OpenSearch (para b√∫squeda y dashboards),
* sink a bases SQL de apoyo (para vistas materializadas).
---
# üìò **5. Introudcci√≥n a MinIO**

**MinIO** es una alternativa *open-source* y ligera al servicio Amazon S3.

Implementa la API oficial de S3, por lo que herramientas como Airbyte, Spark, Kafka Connect o Python pueden trabajar con MinIO exactamente igual que si fuera AWS S3.

### ¬øPara qu√© lo usamos en Big Data?

-   Para simular un **data lake** (zona RAW / STAGED / CURATED).
-   Para almacenar **ficheros Parquet**, **CSV**, **JSON**, logs‚Ä¶
-   Para probar pipelines sin necesidad de usar AWS.

### ¬øPor qu√© lo usamos en clase?

-   No requiere cuenta cloud.
-   Es muy ligero (solo un contenedor Docker).
-   Compatible con los conectores modernos.

------------------------------------------------------------------------

# üìò **6. ¬øQu√© es Zookeeper?**

Tradicionalmente, Kafka depend√≠a de un sistema externo llamado **Zookeeper**, cuyo cometido es:

-   Mantener informaci√≥n de estado de los brokers Kafka
-   Coordinar controladores en el cluster
-   Elegir l√≠deres para particiones
-   Almacenar metadatos de configuraci√≥n

**HOY:** Kafka moderno incluye un modo llamado *KIP-500* que elimina Zookeeper, pero muchas distribuciones de Debezium y Kafka Connect **todav√≠a usan la arquitectura cl√°sica**.

### ¬øPor qu√© aparece en nuestra pr√°ctica?

Porque Debezium + Kafka Connect dependen de la arquitectura tradicional de Kafka, donde Zookeeper es obligatorio. Es importante conocerlo al menos a nivel conceptual.
---

## 7. Ejemplos did√°cticos

### 5.1. Ejemplo simple de productor Kafka en Python

Suponiendo que tenemos un broker Kafka accesible (local o en la nube), podemos enviar mensajes desde Python usando `kafka-python` o `confluent-kafka`. Ejemplo simplificado:

```python
from kafka import KafkaProducer
import json
from datetime import datetime

producer = KafkaProducer(
    bootstrap_servers="localhost:9092",
    value_serializer=lambda v: json.dumps(v).encode("utf-8")
)

evento = {
    "cliente_id": 123,
    "producto_id": 45,
    "cantidad": 2,
    "timestamp": datetime.utcnow().isoformat()
}

producer.send("ventas", value=evento)
producer.flush()
```

Este c√≥digo env√≠a un mensaje JSON al topic `ventas`. En una pr√°ctica, otro script (o un notebook) podr√≠a leer estos eventos y contarlos, agruparlos, etc.

### 5.2. Esquema de integraci√≥n completo

Un flujo moderno de integraci√≥n podr√≠a ser:

1. **Airbyte** copia tablas ¬´lentas¬ª (maestros, cat√°logos) desde PostgreSQL a S3.
2. **Debezium** captura los cambios en las tablas transaccionales (pedidos, pagos) y los env√≠a a Kafka.
3. **Kafka Connect** vuelca esos eventos a otra base de datos o a un *data lake* en tiempo real.
4. **Spark** o **Flink** consumen los datos de Kafka para construir vistas anal√≠ticas o modelos de ML.

Gr√°ficamente:

```text
PostgreSQL  ‚îÄ‚îÄ(Airbyte batch)‚îÄ‚îÄ‚ñ∂  S3 (data lake)
      ‚îÇ
      ‚îî‚îÄ(Debezium CDC)‚îÄ‚îÄ‚ñ∂ Kafka ‚îÄ‚îÄ(Connect sink)‚îÄ‚îÄ‚ñ∂ S3 / DW / Elastic
```

---

## 8. Actividad pr√°ctica: *pipelines* con Airbyte, Debezium y Kafka

Al final de la unidad, se propone una actividad integradora donde el alumnado dise√±ar√° un peque√±o ecosistema moderno de integraci√≥n de datos. La actividad tiene dos variantes para adaptarse a las capacidades de los equipos:

* **Variante A (local con Docker)**: para quienes dispongan de hardware suficiente (m√≠nimo 8 GB RAM recomendados).
* **Variante B (cloud / servicios gestionados)**: para quienes tengan hardware limitado.

### 6.1. Objetivo de la actividad

Dise√±ar y documentar un *pipeline* de datos que combine al menos **dos** de estas piezas:

* Airbyte para ingesta *batch* o incremental,
* Debezium para captura de cambios (CDC),
* Kafka + *consumer* para procesar o visualizar los eventos.

No es necesario desplegar todo el ecosistema completo, pero s√≠ ver en funcionamiento:

* una ingesta peri√≥dica o incremental con Airbyte, o
* un flujo de cambios (CDC) que llegue hasta Kafka, o
* un peque√±o flujo de eventos simulados desde un *producer* Python a Kafka.

### 6.2. Variante A: entorno local (Docker Compose)

Para el que tenga equipo suficientes, se facilitar (o completar en clase) un `docker-compose.yml` con servicios como:

* `zookeeper` + `kafka` (o un Kafka moderno sin Zookeeper),
* `postgres` (como base de datos de ejemplo),
* `debezium` (conector de CDC),
* `airbyte` (opcional),
* `minio` (para simular S3).

El trabajo consistir√° en:

1. Levantar los contenedores.
2. Configurar una conexi√≥n de Airbyte (ej.: PostgreSQL ‚Üí MinIO).
3. Configurar un conector Debezium que env√≠e cambios de una tabla a Kafka.
4. Escribir un peque√±o script Python que lea mensajes del topic y los muestre por pantalla o los escriba en un fichero.

### 6.3. Variante B: entorno *cloud* (sin instalaciones locales pesadas)

Para el alumnado sin capacidad de c√≥mputo suficiente, se puede plantear una alternativa apoyada en cuentas gratuitas o de estudiante (seg√∫n disponibilidad del centro y del curso):

* Uso de **Airbyte Cloud** o instancia desplegada en la nube, accesible desde el navegador.
* Uso de un **cluster Kafka gestionado** (por ejemplo, un servicio con plan gratuito o un cluster preparado ) donde:

  * ya existan credenciales,
  * y solo haya que conectarse con un *producer/consumer* ligero (Python) desde su equipo o desde un notebook en la nube (Google Colab, por ejemplo).

En este caso, las tareas ser√≠an:

1. Acceder a la interfaz *cloud* de Airbyte (o similar) y revisar/construir una conexi√≥n ejemplo (aunque no puedan administrar el servidor).
2. Usar un cuaderno de Python en la nube (Colab) que se conecta al Kafka gestionado (con credenciales proporcionadas) para:

   * enviar algunos eventos de ejemplo,
   * consumirlos y mostrarlos en un gr√°fico o tabla simple.

### 6.4. Entregables comunes

Independientemente de la variante elegida, se entregar√°:

* Un **diagrama del flujo de datos** (puede ser dibujado con una herramienta sencilla o en Markdown usando texto).
* Un breve **informe t√©cnico** (1‚Äì2 p√°ginas) que explique:

  * qu√© herramienta se ha usado para qu√©,
  * qu√© datos se mueven y con qu√© frecuencia,
  * qu√© problemas se han encontrado y c√≥mo se han resuelto.
* Fragmentos de **configuraci√≥n o c√≥digo** utilizados (scripts Python, extractos de configuraci√≥n, etc.).
* Capturas de pantalla clave del pipeline en funcionamiento (Airbyte, Debezium, panel de Kafka, notebooks‚Ä¶ cuando sea posible).

Opcionalmente, se puede a√±adir para esta actividad al final de la unidad:

* dise√±o del pipeline,
* correcta configuraci√≥n de las herramientas,
* documentaci√≥n y claridad de la explicaci√≥n,
* reflexi√≥n sobre ventajas/inconvenientes de la arquitectura elegida.


